{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaL2pq48b9Hx",
        "outputId": "e7a2bae8-35e7-4a83-e8da-0a17e32d8226"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "# Open a CSV file from the shared drive for reading\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4VdUJhd_BDch"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gemini 1.5 Flash"
      ],
      "metadata": {
        "id": "XZX8DGyYBFB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' Gemini API Key\n",
        "*********\n",
        "'''"
      ],
      "metadata": {
        "id": "01EwOx8gBf1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import pandas as pd\n",
        "import subprocess  #send API requests via `curl`.\n",
        "import json\n",
        "import time\n",
        "from tqdm import tqdm  # progress bar\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "# API key\n",
        "GEMINI_API_KEY = \"Insert your key \"\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "# Function to split long interview transcripts into smaller chunks for processing\n",
        "# Possibly where the 34 \"Please provide summary\" came from still summarized\n",
        "def chunk_text(text, max_length=3000):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_length = 0\n",
        "\n",
        "    # Iterates over words and groups them into chunks of specified max_length\n",
        "    for word in words:\n",
        "        if current_length + len(word) + 1 <= max_length:\n",
        "            current_chunk.append(word)\n",
        "            current_length += len(word) + 1\n",
        "        else:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "            current_chunk = [word]\n",
        "            current_length = len(word) + 1\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Function to summarize a single chunk using Gemini API\n",
        "def summarize_chunk(text, retries=3):\n",
        "    for attempt in range(retries):  # Retry mechanism in case of failures\n",
        "        try:\n",
        "            # Gemini Prompt the summarise is running through\n",
        "            prompt = (\n",
        "                \"Summarize the following two-way interview while preserving key proper names. \"\n",
        "                \"Focus on main themes, important takeaways, and discussion points:\\n\\n\" + text\n",
        "            )\n",
        "\n",
        "            # Formatting request payload for API call\n",
        "            payload = json.dumps({\n",
        "                \"contents\": [ { \"parts\": [{ \"text\": prompt }] } ]\n",
        "            })\n",
        "\n",
        "            # Making API request using curl command\n",
        "            curl_command = [\n",
        "                \"curl\", f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={GEMINI_API_KEY}\",\n",
        "                \"-H\", \"Content-Type: application/json\",\n",
        "                \"-X\", \"POST\",\n",
        "                \"-d\", payload\n",
        "            ]\n",
        "\n",
        "            # Executing the command and capturing the response\n",
        "            result = subprocess.run(curl_command, capture_output=True, text=True)\n",
        "            response = json.loads(result.stdout)\n",
        "\n",
        "            # Extracting the summarized text from API response\n",
        "            summary = response.get(\"candidates\", [{}])[0].get(\"content\", {}).get(\"parts\", [{}])[0].get(\"text\", \"\").strip()\n",
        "            if summary:\n",
        "                return summary\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Attempt {attempt + 1}: Error during summarization: {e}\")\n",
        "\n",
        "        time.sleep(5)  # Wait 5 seconds before retrying\n",
        "\n",
        "    print(\"‚ùå Failed to summarize after retries.\")\n",
        "    return None  # Return None if all retries fail\n",
        "\n",
        "# Function to generate summary for an entire interview transcript\n",
        "def generate_summary(text):\n",
        "    chunks = chunk_text(text, max_length=3000)  # Splitting long text if necessary\n",
        "    summaries = [summarize_chunk(chunk) for chunk in chunks]  # Summarizing each chunk\n",
        "\n",
        "    # Combine all chunk summaries into a final summary\n",
        "    final_summary = \" \".join([s for s in summaries if s])\n",
        "    return final_summary if final_summary else None\n",
        "\n",
        "# Load interview transcripts from CSV\n",
        "csv_path = '/content/drive/MyDrive/Colab Notebooks'\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Ensure 'Summary' column exists in the DataFrame\n",
        "if 'Summary' not in df.columns:\n",
        "    df['Summary'] = None\n",
        "\n",
        "# Tracking API request limits and batch processing\n",
        "batch_count = 0  # Count requests per minute\n",
        "total_requests = 0  # Count total API requests\n",
        "BATCH_LIMIT = 15  # Gemini 1.5 Flash allows 15 requests per minute\n",
        "TOTAL_LIMIT = 1500  # Stop processing after 1500 requests\n",
        "\n",
        "# Filter rows that still need summarization\n",
        "remaining_rows = df[df['Summary'].isna() | (df['Summary'].str.strip() == \"\")]\n",
        "\n",
        "print(f\"üîç Found {len(remaining_rows)} interviews that still need summaries.\")\n",
        "\n",
        "# Iterate over remaining rows and generate summaries\n",
        "for index, row in tqdm(remaining_rows.iterrows(), total=len(remaining_rows), desc=\"Summarizing Interviews\"):\n",
        "    text = row.get('RecordingTranscription_1', None)\n",
        "\n",
        "    if isinstance(text, str) and text.strip():  # Ensure text is valid\n",
        "        summary = generate_summary(text)\n",
        "        if summary:\n",
        "            df.at[index, 'Summary'] = summary  # Save summary in DataFrame\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Warning: Failed to generate summary for row {index}, skipping.\")\n",
        "\n",
        "        batch_count += 1\n",
        "        total_requests += 1\n",
        "\n",
        "    else:\n",
        "        print(f\"Skipping row {index} (Invalid text)\")\n",
        "\n",
        "    # Save progress periodically to prevent data loss\n",
        "    if batch_count >= BATCH_LIMIT:\n",
        "        print(\"üíæ Saving progress...\")\n",
        "        df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
        "\n",
        "        print(\"‚è≥ Waiting 60 seconds before processing the next batch...\")\n",
        "        time.sleep(60)  # Wait before processing next batch\n",
        "        batch_count = 0\n",
        "\n",
        "    # Stop after reaching API request limit\n",
        "    if total_requests >= TOTAL_LIMIT:\n",
        "        print(\"üöÄ Reached 1500 requests. Please resume processing on the next day.\")\n",
        "        break\n",
        "\n",
        "# Final save after completing summarization\n",
        "df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
        "\n",
        "print(f\"‚úÖ Summarized data saved to {csv_path}\")\n"
      ],
      "metadata": {
        "id": "fGZqCJdXYI8w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "77df09f4-9652-44db-a0cb-d7a97457ac71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Found 2157 interviews that still need summaries.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
           
