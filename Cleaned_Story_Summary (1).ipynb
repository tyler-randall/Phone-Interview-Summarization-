{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaL2pq48b9Hx",
        "outputId": "b6905b2c-4ead-45b5-8e5c-96ad3218ae20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "# Open a CSV file from the shared drive for reading\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4VdUJhd_BDch"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gemini 1.5 Flash"
      ],
      "metadata": {
        "id": "XZX8DGyYBFB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' Gemini API Key\n",
        "AIzaSyAkTfXVMMSSuO6FCf8Y7F9b_-pqb15IERw\n",
        "'''"
      ],
      "metadata": {
        "id": "01EwOx8gBf1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import pandas as pd\n",
        "import subprocess  # Send API requests via `curl`\n",
        "import json\n",
        "import time\n",
        "from tqdm import tqdm  # Progress bar\n",
        "from google.colab import drive\n",
        "\n",
        "# API Key\n",
        "GEMINI_API_KEY = \"AIzaSyAkTfXVMMSSuO6FCf8Y7F9b_-pqb15IERw\"\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Function to split long transcripts into smaller chunks\n",
        "def chunk_text(text, max_length=3000):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_length = 0\n",
        "\n",
        "    for word in words:\n",
        "        if current_length + len(word) + 1 <= max_length:\n",
        "            current_chunk.append(word)\n",
        "            current_length += len(word) + 1\n",
        "        else:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "            current_chunk = [word]\n",
        "            current_length = len(word) + 1\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Function to summarize a single chunk using Gemini API\n",
        "def summarize_chunk(text, retries=3):\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            # Gemini API Prompt\n",
        "            prompt = (\n",
        "                \"Summarize the following two-way interview while preserving key proper names. \"\n",
        "                \"Focus on main themes, important takeaways, and discussion points:\\n\\n\" + text\n",
        "            )\n",
        "\n",
        "            # Request payload for API\n",
        "            payload = json.dumps({\n",
        "                \"contents\": [ { \"parts\": [{ \"text\": prompt }] } ]\n",
        "            })\n",
        "\n",
        "            # Making API request using curl\n",
        "            curl_command = [\n",
        "                \"curl\", f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={GEMINI_API_KEY}\",\n",
        "                \"-H\", \"Content-Type: application/json\",\n",
        "                \"-X\", \"POST\",\n",
        "                \"-d\", payload\n",
        "            ]\n",
        "\n",
        "            # Execute the command and capture the response\n",
        "            result = subprocess.run(curl_command, capture_output=True, text=True)\n",
        "            response = json.loads(result.stdout)\n",
        "\n",
        "            # Extract summarized text from API response\n",
        "            summary = response.get(\"candidates\", [{}])[0].get(\"content\", {}).get(\"parts\", [{}])[0].get(\"text\", \"\").strip()\n",
        "\n",
        "            if summary:\n",
        "                return summary\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Attempt {attempt + 1}: Error during summarization: {e}\")\n",
        "\n",
        "        time.sleep(5)  # Retry delay\n",
        "\n",
        "    print(\"‚ùå Failed to summarize after retries.\")\n",
        "    return None\n",
        "\n",
        "# Function to generate a summary for a full transcript\n",
        "def generate_summary(text):\n",
        "    chunks = chunk_text(text, max_length=3000)\n",
        "    summaries = [summarize_chunk(chunk) for chunk in chunks]\n",
        "\n",
        "    final_summary = \" \".join([s for s in summaries if s])  # Join non-empty summaries\n",
        "    return final_summary if final_summary else None\n",
        "\n",
        "# Load CSV from Google Drive\n",
        "csv_path = \"/content/drive/MyDrive/Colab Notebooks/SUU_AllStories_02_24_2024/Copy of Final_SUU_AllStories_02_24_2024_14_06_41_PM_Includes_SUMMARIES.csv\"\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Ensure 'Summary' column exists\n",
        "if 'Summary' not in df.columns:\n",
        "    df['Summary'] = None\n",
        "\n",
        "# Filter rows that still need summarization\n",
        "remaining_rows = df[df['Summary'].isna() | (df['Summary'].str.strip() == \"\")]\n",
        "\n",
        "print(f\"üîç Found {len(remaining_rows)} interviews that need summaries.\")\n",
        "\n",
        "# API limits\n",
        "BATCH_LIMIT = 15  # Gemini 1.5 Flash allows 15 requests per minute\n",
        "TOTAL_LIMIT = 1500  # Stop after 1500 requests\n",
        "batch_count = 0\n",
        "total_requests = 0\n",
        "updated = False  # Track if we need to save\n",
        "\n",
        "# Process remaining rows\n",
        "for index, row in tqdm(remaining_rows.iterrows(), total=len(remaining_rows), desc=\"Summarizing Interviews\"):\n",
        "    text = row.get('RecordingTranscription_1', None)\n",
        "\n",
        "    if isinstance(text, str) and text.strip():\n",
        "        summary = generate_summary(text)\n",
        "        if summary:\n",
        "            df.at[index, 'Summary'] = summary\n",
        "            updated = True  # Mark that we made changes\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Warning: Failed to generate summary for row {index}, skipping.\")\n",
        "\n",
        "        batch_count += 1\n",
        "        total_requests += 1\n",
        "\n",
        "    else:\n",
        "        print(f\"Skipping row {index} (Invalid text)\")\n",
        "\n",
        "    # Save progress every batch and prevent API limit issues\n",
        "    if batch_count >= BATCH_LIMIT:\n",
        "        if updated:\n",
        "            print(\"üíæ Saving progress...\")\n",
        "            df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
        "            updated = False  # Reset update flag\n",
        "\n",
        "        print(\"‚è≥ Waiting 60 seconds before processing the next batch...\")\n",
        "        time.sleep(60)\n",
        "        batch_count = 0\n",
        "\n",
        "    # Stop after reaching the request limit\n",
        "    if total_requests >= TOTAL_LIMIT:\n",
        "        print(\"üöÄ Reached 1500 requests. Please resume processing tomorrow.\")\n",
        "        break\n",
        "\n",
        "# Final save if any updates were made\n",
        "if updated:\n",
        "    df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
        "\n",
        "print(f\"‚úÖ Summarized data saved to {csv_path}\")\n"
      ],
      "metadata": {
        "id": "fGZqCJdXYI8w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05e3eee4-e2ec-4d66-c04a-f7337adb5a6b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "üîç Found 0 interviews that need summaries.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Summarizing Interviews: 0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Summarized data saved to /content/drive/MyDrive/Colab Notebooks/SUU_AllStories_02_24_2024/Copy of Final_SUU_AllStories_02_24_2024_14_06_41_PM_Includes_SUMMARIES.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}